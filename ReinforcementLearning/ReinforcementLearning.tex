\chapter{Reinforcement Learning}
\section{Motivation}
In the last chapter, we discussed the \textit{Markov Decision Process (MDP)}: a framework that models a learner's environment as a vector of states, actions, rewards, and transition probabilities. Given this model, we can solve for an optimal (reward-maximizing) policy using either value iteration or policy iteration. Sometimes, however, the learner doesn't have prior knowledge of the rewards they will get from each state or the probability distribution over states they could end up in after taking some action from their current state. Is it still possible to learn a policy that will maximize rewards? In this chapter, we will learn about \textbf{Reinforcement Learning (RL)} - a machine learning technique that addresses this problem.
\begin{mlcube}{Reinforcement Learning}
\begin{center}
    \begin{tabular}{c|c|c}
    \textit{\textbf{Domain}} & \textit{\textbf{Training}} & \textit{\textbf{Probabilistic}} \\
    \hline
    Discrete or Continuous & ? & Yes \\
    \end{tabular}
\end{center}
Note that we will only cover RL for discrete state spaces in this chapter. The question mark under the ``training" category means that RL is neither supervised nor unsupervised. 
\end{mlcube}
\section{General Approaches to RL}
One way to solve the problem described in the previous section is to try and \textit{learn} the MDP. In other words, we can create a learner that will explore its environment for some amount of time in order to learn the rewards associated with each state and the transition probabilities associated with each action. Then, they can use an MDP planning algorithm like policy iteration or value iteration to come up with an optimal policy and maximize their rewards. This is called \textbf{model-based} learning. The main advantage of model-based learning is that it can inexpensively incorporate changes in the reward structure or transition function of the environment into the model. However, model-based learning is computationally expensive, and an incorrect model can yield a sub-optimal policy.\\

Conversely, \textbf{model-free} learning is a family of RL techniques where a learner tries to learn the optimal policy directly without modelling the environment. While this sort of learning is less adaptive to changes in the environment, it is far more computationally efficient than model-based learning.\\

Note that in both types of RL, a key question is finding a balance between time spent \textbf{exploring} the environment and \textbf{exploiting} what one has learned to accumulate the highest possible reward. 
\section{Model-Free Learning}
In this section we will cover \textbf{value-based} methods for model-free reinforcement learning. In this family of RL algorithms, the learner tries to calculate the expected reward they will receive from a state $s$ upon taking action $a$. Formally, they are trying to learn a function that maps $(s, a)$ to some value representing the expected reward. This is called the Q-function and is defined as follows for some policy $\pi$:
\begin{equation}
    Q^{\pi}(s, a) = r(s, a) + \gamma\sum_{s'}p(s'|s, a)V^{\pi}(s')
\end{equation}
In words, the approximate expected reward (Q value) for taking action $s$ from state $a$ is the actual reward received from the environment by doing so in the current iteration plus the expectation taken over all reachable states of the highest value achievable starting at that state times the discount factor $\gamma$. The Q-function following the optimal policy is defined analogously:
\begin{equation}
    Q^{*}(s, a) = r(s, a) + \gamma\sum_{s'}p(s'|s, a)V^{*}(s')
\end{equation}
Note, that $V^*(s') = max_{a'}Q^*(s', a')$ since the highest value achievable from state $s'$ following policy $*$ is the Q value of taking the optimal action from $s'$. Substituting this in, we get the following \textit{Bellman Equation}:
\begin{equation}
    Q^*(s, a) = r(s, a) + \gamma\sum_s'p(s'|s, a)\max_{a'}Q^*(s', a)
\end{equation}
Note that we can't directly calculate the term $\gamma\sum_s'p(s'|s, a)\max_a'Q^*(s', a)$ since we don't know $p(s'|s, a)$. We will discuss how this is addressed by the two algorithms we will cover in the value-based family.
\subsection{SARSA and Q-Learning}
At a high level value-based model-free reinforcement learners work by initializing the values of $Q(s, a)$ for all states $s$ and all actions $a$ in an $s$ x $a$ matrix. They then repeat the following two steps until satisfactory performance is achieved: act based on Q values and then use $s$ (current state), $a$ (action), $r$ (reward), $s'$ (next state), $a'$ (action taken from next state) in order to update the approximation of $Q(s, a)$. We will refer to $(s, a, r, s', a')$ as an \textbf{experience}. The action that the learner takes from state $s$, $\pi(s)$ is defined as follows:
\begin{equation}
    \pi(s) = 
    \begin{cases} 
      \arg\!\max_aQ(s, a) \text{ with probability 1 - }\epsilon \\
      \text{random with probability }\epsilon
    \end{cases}
\end{equation}
Here, $\epsilon$ is some number $\in [0, 1]$ which controls how likely the learner is to choose a random action as opposed to the currently known optimal action. Varying the value of $\epsilon$ changes the balance between exploration and exploitation. A learner that behaves as described above is called $\epsilon$\textbf{-greedy}.\\
Once the learner has had an experience, they can begin to learn $Q^*$. 
\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.2]{cs181-textbook/ReinforcementLearning/model-free.jpeg}
    \caption{The process of model-free learning}
\end{figure}
We will now describe two algorithms which perform this update differently for every new experience:
\begin{enumerate}
    \item \textbf{SARSA}: $Q(s, a) \leftarrow \alpha_t(s, a)[r + \gamma Q(s', a') - Q(s, a)]$ where $\alpha_t(s, a)$ is the \textbf{learning rate}, a parameter which controls how much the observation affects $Q(s, a)$.\\
    The expression $r + \gamma Q(s', a')$ is a 1-step estimate of $Q(s, a)$. The expression in the square brackets above is called the \textbf{temporal difference (TD) error} and represents the difference between the previous estimate of $Q(s, a)$ and the new one. Since the action $a'$ that we use for our update is the one that was recommended by the policy $\pi$ (recall that $a' = \pi(s')$), SARSA is an \textbf{on-policy} algorithm.
    \item \textbf{Q-learning}: $Q(s, a) \leftarrow Q(s, a) + \alpha_t(s, a)[r + \gamma\max_a'(s', a') - Q(s, a)]$\\
    Similar to SARSA, the expression in the square brackets is the TD error. Note, that in Q-learning the update depends on the reward-maximizing action $\max_a'(s', a')$ corresponding to the Bellman equation $Q^*$ rather than the policy-recommended action. This makes it an \textbf{off-policy} algorithm.
\end{enumerate}
This procedure of learning $Q$ by updating $Q(s, a)$ by the TD error is called \textbf{TD updating}.
\subsubsection{Convergence Conditions}
Let $\alpha_t(s, a) = 0$ for all $(s, a)$ that are not visited at time $t$. It is a theorem that Q-learning converges to $Q^*$ (and hence $\pi$ converges to $\pi^*$) as $t \rightarrow \infty$ as long as the following two conditions are met:
\begin{itemize}
    \item $\sum_t\alpha_t(s, a) = \infty$ $\forall s, a$\\
    The sum of the learning rate over infinitely many time steps must diverge. In order for this to happen, each state-action pair $(s, a)$ must be visited infinitely often. Thus, we see the importance of an $\epsilon$-greedy learner which forces the agent to probablistically take random actions in order to explore more of the state space.
    \item $\sum_t\alpha(s, a)^2 < \infty$ $\forall s, a$\\
    The sum of the square of the learning rate over infinitely many time steps must converge. Note that in order for $\alpha_t(s, a)^2 < \infty$, the learning rate must be iteratively reduced. For example, we could set $\alpha_t(s, a) = \frac{1}{N_t(s, a)}$ where $N_t(s, a)$ is the number of times the learner took action $a$ from state $s$.
\end{itemize}
SARSA converges to $Q^*$ if the above two conditions are met \textit{and} behavior is greedy in the limit (ie. $\epsilon \rightarrow 0$ as $t \rightarrow \infty$). One common choice is $\epsilon_t(s) = \frac{1}{N_t(s)}$ where $N_t(s)$ is the number of times the learner visited state $s$. The notation $\epsilon_t(s)$ implies that a separate $\epsilon$ value is maintained for every state rather than just maintaining one value of $\epsilon$ that controls learner behavior across \textit{all} states (though this is also an option).
\subsection{Deep Q-Networks}
Sometimes, we have scenarios where there are too many state-action pairs to learn via traditional TD updates in a reasonable amount of time. For example, consider the game of chess where there are mind bogglingly many possible configurations of the board. In cases like these, we can use a universal function approximator, like a convolutional neural network, to estimate the Q-function. The neural network takes the learner's current state as input and outputs the approximated Q-values for taking each possible action from that state by training parameters $w$. The learner's next action is the maximum over all outputs of the neural network.
\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.6]{cs181-textbook/ReinforcementLearning/deep-q-learning.png}
    \caption{Q-learning vs Deep Q-learning.(Image citation: https://www.mlq.ai/deep-reinforcement-learning-q-learning/)}
    \label{fig:my_label}
\end{figure}\\
The loss function that the neural network tries to minimize at iteration $i$ is as follows:
\begin{equation}
    \mathcal{L}(w_i) = E[(r + \gamma\max_{a'}Q(s', a'; w_{i-1}) - Q(s, a; w_i))^2]
\end{equation}

Here, $(s, a, r, s', a')$ and $\gamma$ are defined the same as in regular Q-learning. $w_i$ are the parameters that the neural net is training during the current iteration of the RL algorithm, and $w_{i-1}$ are the optimal parameters from the previous iteration of the algorithm. The TD error is the term $r + \gamma\max_{a'}Q(s', a';w_{i-1})$, and the squared term inside the expectation is the TD target. Rather than having to calculate the entire expected value term in the above loss function, we can minimize it using stochastic gradient descent.\\

The Atari deep Q network that solved the game of brickbreaker used a technique called \textit{experience replay} to make updates to the network more stable. The experience $(s, a, r, s')$ was put into a \textit{replay buffer} which was sampled from to perform minibatch gradient descent to minimize the loss function.
\subsection{Policy Learning}
There are a few situations in which the RL techniques described thus far don't work well or work relatively less well than alternatives:
\begin{enumerate}
    \item The Q function is far more complex than the policy being learned.
    \item The action space is continuous.
    \item Expert knowledge needs to be introduced.
\end{enumerate}
This presents the need for another model-free method called \textbf{policy learning} in which we adopt a differentiable policy $\pi_\theta(a|s)$ parametrized by $\theta$, and try to learn $\theta$ directly without going through a Q function first.\\
We update $\theta$ iteratively according to the following equation:
\begin{equation*}
    \theta \leftarrow \theta + \alpha_t \nabla_\theta J(\theta)
\end{equation*}
where $J(\theta) = E[r(h)]$, $r(h) = \sum_tr_t$, and $h = (s, a, r, s', a', r', s'', a''...)$. The gradient term approximates the performance $V^{\pi}$. We define the likelihood $\mu_\theta(h) = \prod_t\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a)$. We can expand $\nabla_\theta(\theta)$ as follows:
\begin{equation}
    \nabla_\theta J(\theta) = \nabla_\theta E[r(n)] = \nabla_\theta\int_h\mu_\theta(h)r(h)\delta h = \int_hr(h)\nabla_\theta\mu_\theta(h)\delta h
\end{equation}
We seem to have run into an issue here. The term $\nabla_\theta\mu_\theta(h)$ depends on the transition probability $p(*|s, a)$ by definition, but point of policy learning was to avoid having to learn these transition probabilities. However, it turns out we can circumvent this issue by applying the useful identity
\begin{equation}
    \nabla_\theta\mu_\theta = \mu_\theta\frac{1}{\mu_\theta}\nabla_\theta\mu_\theta = \mu_\theta\nabla_\theta\ln\mu_\theta
\end{equation}
Thus, we can rewrite the previous equation as follows:
\begin{align}
    \int_hr(h)\mu_\theta(h)\nabla_\theta\left[\sum_t\ln\pi_\theta(a_t|s_t) + \sum_t\ln p(s_{t+1}|s_t, a)\right]\delta h\\
    = E\left[r(h)\sum_t\nabla_\theta\ln\pi_\theta(a_t|s_t)\right]\\
    = E\left[\sum_t\nabla_\theta\ln\pi_\theta(a_t|s_t)r_t\right]
\end{align}
This expression only uses values we have access to. We can now perform SGD to find the optimal $\theta$. Note policy learning is on-policy.
\section{Model-Based Learning}
\begin{figure}[ht!]
    \centering
    \includegraphics[scale=0.2]{cs181-textbook/ReinforcementLearning/model-based.jpeg}
    \caption{The process of model-based learning.}
\end{figure}

Recall that in model-based learning, the learner tries to learn the parameters of the MDP underlying their environment and then uses planning to come up with a policy. As with model-free learning, a key challenge in model-based learning is balancing exploration and exploitation.\\

One approach to learning the underlying MDP is to employ \textbf{optimism under uncertainty}. Let $N(s, a)$ be the number of times the state-action pair $(s, a)$ was visited. If $N(s, a)$ is small, we assume that the state $s'$ that we end up in next will have a high reward.\\

Once we have approximated a model, we must plan in order to come up with a policy to execute for the next $M$ rounds. However, there is still one key piece of information that is unknown: the transition probabilities $p(s'|s, a)$. One approach to approximate these probabilities is called \textbf{posterior} or \textbf{Thompson sampling} in which we start by updating the prior probabilities of $p(s'|s, a)$ $\forall s', s, a$ to posterior probabilities (for example, as a Dirichlet distribution) given the data observed during the learner's previous action phase. The more times the learner ended up in state $s'$ by taking action $a$ from state $s$, the higher the posterior probability $p(s'|s, a)$. We then sample from this probability distribution to decide which action to play during the MDP planning phase.
\section{Conclusion}
To recap, reinforcement learning is a machine learning technique that allows a learner to find an optimal policy in an environment modeled by an MDP where the transition probabilities and reward function are unknown. The learner gains information by interacting with the environment and receiving rewards and uses this information to update the model they have of the underlying MDP (model-based) or their beliefs about the value of taking particular actions from particular states (value-based model-free). Q-learning and temporal difference updating are key topics in model-free learning that underlie many techniques in the area including SARSA, Q-learning and deep Q-learning. Contemporary RL models such as the one used by AlphaGo often use a combination of supervised and reinforcement learning methods which can be endlessly customized to meet the needs of a problem.